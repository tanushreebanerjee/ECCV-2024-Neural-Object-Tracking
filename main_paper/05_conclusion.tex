\section{Conclusion}
We investigate inverse neural rendering as an alternative to existing feed-forward tracking methods. Specifically, we recast 3D multi-object tracking from RGB cameras as an inverse test-time optimization problem over the latent space of pre-trained 3D object representations that, when rendered, best represent object instances in a given input image. We optimize an image loss over generative latent spaces that inherently disentangle shape and appearance properties. This approach to tracking also enables examining the reconstructed objects, reasoning about failure situations, and resolving ambiguous cases -- rendering object layouts and loss function values provides interpretability ``for free''. We validate that the method has high generalization capabilities, and without seeing a dataset, outperforms existing tracking methods' generalization capabilities. In the future, we hope to investigate not only object detection with inverse rendering but broad, in-the-wild object class identification via conditional generation methods -- unlocking analysis-by-synthesis in vision with generative neural rendering.