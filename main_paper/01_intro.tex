\section{Introduction}
The most successful image understanding methods today employ feed-forward neural networks for performing vision tasks, including segmentation~\cite{long2015fully,li2017fully, chen2014semantic}, object detection~\cite{ren2015faster, girshick2015fast, redmon2016yolo, liu2016ssd, ku2018joint, qi2018frustum, zhou2018voxelnet}, object tracking~\cite{sharma2018beyond, kim2021eagermot, zhou2020CenterTrack, chaabane2021deft, yin2021center, weng2020AB3DMOT,pang2021simpletrack} and pose estimation~\cite{wang2019densefusion, xiang2017posecnn}. Typically, these approaches learn network weights using large labeled datasets. At inference time, the trained network layers sequentially process a given 2D image to make a prediction. Despite being a successful approach across disciplines, from robotics to health, and effective in operating at real-time rates, this approach also comes with several limitations: (i) Networks trained on data captured with a specific camera/geography 
\emph{generalize poorly}, (ii) they typically rely on high-dimensional internal feature representations which are \emph{often not interpretable}, making it hard to identify and reason about failure cases, and, (iii) it is challenging to enforce 3D geometrical constraints and priors during inference.

We focus on multi-object tracking as a task that must tackle all these challenges. Accurate multi-object tracking is essential for safe robotic planning. While approaches using LiDAR point clouds (and camera image input) are successful as a result of the explicitly measured depth \cite{pang2021simpletrack,yin2021center,kim2021eagermot,liu2022bevfusion, weng2020gnn3dmot, focalformer3d, bai2021pointdsc}, camera-based approaches to 3D multi-object tracking have only been studied recently~\cite{hu2021QD3DT, wu2021trades, zhou2020CenterTrack, marinello2022triplettrack, chaabane2021deft, nguyen2022multiCamMultiTrack, gladkova2022directtracker, yang2022qtrack,pang2023PFtrack, wang2023StreamPETR}. Monocular tracking methods, typically consisting of independent detection, 3D dynamic models, and matching modules, often struggle as the errors in the distinct modules tend to accumulate. Moreover, wrong poses in the detections can lead to ID switches in the matching process.

We propose an alternative approach that recasts visual inference problems as inverse rendering (IR) tasks, jointly solving them at test time by optimizing over the latent space of a generative object representation. Specifically, we combine object retrieval through the inversion of a rendering pipeline and a learned object model with a 3D object tracking pipeline. This approach allows us to simultaneously reason about an object's 3D shape, appearance, and three-dimensional trajectory from monocular image input only. The location, pose, shape, and appearance parameters corresponding to the anchor objects are then iteratively refined via test-time optimization to minimize the distance between their corresponding generated objects and the given input image. Rather than directly predicting scene and object attributes, we optimize over a latent object representation to synthesize image regions that best explain the observed image. We match the inverse-rendered objects then be matched by comparing their optimized latents. 

Our method hinges on an efficient rendering pipeline and generative object representation at its core. While the approach is not tied to a specific object representation, we adopt GET3D ~\cite{gao2022get3d} as the generative object prior, that \emph{is only trained on synthetic data} to synthesize textured meshes and corresponding images with an efficient differentiable rendering pipeline. Note that popular implicit shape/object representations do either not support class-specific priors~\cite{park2019deepsdf, mildenhall2020nerf}, or require expensive volume sampling~\cite{shen2023gina3d}. 

The proposed method builds on the inductive geometry priors embedded in our rendering forward model, solving \emph{different several tasks simultaneously}. %For instance, multi-object tracking, shape and texture retrieval, and object pose estimation -- typically considered disjoint tasks -- are jointly solved by our method by optimizing over object rendering parameters such as 3D location and pose. 
Our method refines object pose as a byproduct, merely by learning to represent objects of a given class. Recovering object attributes as a result of inverse rendering also provides \emph{interpretability ``for free''}: once our proposed method detects an object at test time, it can extract the parameters of the corresponding representation alongside the rendered input view. This ability allows for reasoning about failure cases.

We validate that the method naturally exploits 3D geometry priors and \emph{generalizes across unseen domains and unseen datasets}. After training solely on simulated data, we test on nuScenes \cite{caesar2020nuscenes} and Waymo~\cite{sun2020scalability} datasets, and although untrained, we find that our method \emph{outperforms both existing dataset-agnostic multi-object tracking approaches and dataset-specific learned approaches~\cite{zhou2020CenterTrack} when operating on the same detection inputs.}
% par with existing 3D multi-object tracking methods ~\cite{hu2021QD3DT,wang2023StreamPETR, yang2022qtrack} on monocular image data.
In summary, we make the following contributions. 
\begin{itemize}
    \item We introduce an inverse rendering method for 3D-grounded monocular multi-object tracking. Instead of formulating tracking as a feed-forward prediction problem, we propose to solve an inverse image fitting problem optimizing over the latent embedding space of generative scene representations.
    \item We analyze the single-shot capabilities and the interpretability of our method using the generated image produced by our method during test-time optimization.
    \item  Trained only on synthetic data, we validate the generalization capabilities of our method by evaluating on unseen automotive datasets, where the method compares favorably to existing methods when provided the same detection inputs.
\end{itemize}

\subsubsection{Scope and Limitations} While facilitating inverse rendering, the iterative optimization in our method makes it slower than classical object-tracking methods based on feed-forward networks. We hope to address this limitation in the future by accelerating the forward and backward passes with adaptive level-of-detail rendering techniques.

% Intro's skeleton:
% \begin{enumerate}
%     \item Conventional tasks like object detection and pose estimation have been solved with feed-forward neural nets that predict object class location, pose, etc. directly by processing feature maps sequentially. The operators in the sequential layers are learned by fitting the predictions to a large dataset.
%     \item As an immediate result, existing methods do not generalize (all priors have to be learned as image processing operations without geometric priors available), they are not interpretable (produce feature maps instead of images consistent with the predictions), and they do not directly support scene understanding tasks or fusion (2D and 3D detection, segmentation, or stereo all require separate models).
%     \item We recast these vision tasks (3D detection, segmentation) as an inverse rendering task, solving a test-time optimization problem over a learned image prior that is a neural rendering model.
%     \item Advantages of this approach: generalizability (strong domain transfer), 0-shot learning, interpretability, explainability (image output), fusion and jointly solving scene understanding tasks (becomes just another loss term, not different model), provide confidence score on top of explainable output.
%     \item Our approach requires a strong and efficient rendering model backend. To this end, we introduce a novel SDF-based model, that alleviates the need to render 3D volumes, which is very costly. The feature-based texture is 0-shot learned.
%     \item Our method easily generalizes across datasets, as we show for the case of automotive datasets. Unlike conventional methods, our approach can easily be extended to support different or even multiple modalities, as we show in the case of a stereo camera pair. 
%     \item To realize our proposed approach, we introduce an inverse rendering object detection algorithm (some high-level details).
%     \item Limitations of our approach - takes more time (in the future can be improved by...)
%     \item We compare our method to... and demonstrate its advantage over classic 2D and 3D object detection\jo{, instance segmentation} methods \jo{and we further show single shot reconstruction}. 
% \end{enumerate}

% removed contributions:
    % \item We introduce a novel implicit representation capable of rendering a class of objects (e.g., cars) by conditioning on a latent code input. The representation models latent appearance with a unit-sphere texture disentangled from the SDF-parameterized shape.
    %\item We propose an algorithm for performing object tracking over multiple frames given an initial detection using our proposed approach.
    % \item Unlike classic approaches that only return detected object locations, ours additionally returns ``for free'' their poses and their rendered appearance, and further allows rendering novel object views.
    % \item In addition, our inverse-rendering approach returns a confidence score based on the likelihood of the object's corresponding latent code which together with the rendered object allows for assessing detection quality in real-time.
    % \item Our rendering model disentangles the \yb{x,y,z} properties.

    %Our proposed approach additionally allows us to produce confidence scores corresponding to the predictions provided that the latent space of the underlying rendering model is regularized during training.

    % The most successful methods for image understanding make predictions about the objects in an image by evaluating a feed-forward neural network, including 